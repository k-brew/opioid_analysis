---
title: "Exploratory Data Analysis - Trends in Opioid Prescriptions"
author: "Kyle Brewster"
output: html_document
---
<body style="background-color:lightgrey;">

```{r setup, include=FALSE}
# synced with Github
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache=TRUE) ## Cache the results to increase performance.
```

# Data Cleaning & Setup
```{r loading}
library(pacman)
p_load(dplyr, readr, skimr, ggplot2, tidyverse, tidymodels, janitor, magrittr)

opioid_list = read.csv("opioids.csv")
overdose_df = read.csv("overdoses.csv")
prescriber_df = read.csv("prescriber_info.csv")
```

## Overview
```{r skim}
#skim(prescriber_df)
```

## Cleaning up
### The `Credentials` column
```{r cred_vari., message=FALSE, warning=FALSE}
# To get an idea of some of the more-common types of credentials
top_counts(prescriber_df$Credentials, max_levels = 20)

# Cleaning up the acronyms for some of the most common types of credential
scribe_MD <- prescriber_df %>% 
  group_by(Credentials) %>%
  filter(Credentials == "MD"|
         Credentials == "M.D."|
         Credentials == "M.D"|
         Credentials == "M.D.,"|
         Credentials == "M,D"|
         Credentials == "MD."|
         Credentials == "M.D,"|
         Credentials == "M.D")
scribe_DDS <- prescriber_df %>% 
  filter(Credentials == "DDS"|
         Credentials == "DDS, MS,"|
         Credentials == "D.D.S."|
         Credentials == "DDS, MS"|
         Credentials == "DDS. MS"|
         Credentials == "MD, DDS"|
         Credentials == "DDS,MD"|
         Credentials == "DMD"| 
         Credentials == "D.M.D."|
         Credentials == "D.M.D"|
         Credentials == "DDS, MD")
scribe_DO <- prescriber_df %>% 
  group_by(Credentials) %>%
  filter(Credentials == "DO"|
         Credentials == "D.O."|
         Credentials == "D.O"|
         Credentials == "DO,"|
         Credentials == "D.O., M.D"|
         Credentials == "D.O., M.D."|
         Credentials == "D,O."|
         Credentials == "D.O.,")
scribe_PA <- prescriber_df %>% 
  group_by(Credentials) %>%
  filter(Credentials == "PA"|
         Credentials == "P.A."|
         Credentials == "P.A.-C"|
         Credentials == "PA-C"|
         Credentials == "P.A"|
         Credentials == "PA-"|
         Credentials == "PA,"|
         Credentials == "PA."|
         Credentials == "P,A."|
         Credentials == "P.A.C"|
         Credentials == "P.A.,")  
scribe_NP <- prescriber_df %>% 
  group_by(Credentials) %>%
  filter(Credentials == "NP"|
         Credentials == "N.P."|
         Credentials == "N.P"|
         Credentials == "NP."|
         Credentials == "NP,"|
         Credentials == "N.P.,"|
         Credentials == "N P"|
         Credentials == "NURSE PRACTITIONER"|
         Credentials == "NP-C"|
         Credentials == "NP."|
         Credentials == "N P")
    ## Now to make the values more uniform
scribe_DDS = replace(scribe_DDS, 4,"DDS_DDM")
scribe_DO  = replace(scribe_DO, 4, "DO")
scribe_MD  = replace(scribe_MD, 4, "MD")
scribe_PA  = replace(scribe_PA, 4, "PA")
scribe_NP  = replace(scribe_NP, 4, "NP")
```

```{r merging}
scribe_joint = rbind(scribe_DDS, scribe_DO,
                     scribe_MD, scribe_NP,
                     scribe_PA)
```

### The `Specialty` column
Looking at the top specialties
```{r top_counts}
tab <- as.data.frame(table(prescriber_df$Specialty))
tab =arrange(tab, desc(Freq)) 
```
Filtering the data to only some the more-common values
```{r top_count}
scribe_joint = scribe_joint %>%
  group_by(Specialty) %>%
  filter(Specialty == "Internal Medicine"|
         Specialty == "Family Practice"|
         Specialty == "Dentist"|
         Specialty == "Physician Assistant"|
         Specialty == "Nurse Practitioner"|
         Specialty == "Emergency Medicine"|
         Specialty == "Psychiatry"|
         Specialty == "Cardiology"|
         Specialty == "Obstetrics/Gynecology"|
         Specialty == "Orthopedic Surgery"|
         Specialty == "Optometry"|
         Specialty == "General Surgery")
```

### The State Naming Convention
Using the `state.abb` package to save some effort in narrowing this set down to 50 states and excluding territories, miscellaneous, or missing categories
```{r}
list_of_states = as.data.frame(cbind(state.abb))
list_of_states = list_of_states %>%
  mutate(State = state.abb) # Let's make the state variable have the same naming convention in both; and then filter out providers outside of our scope of interest
scribe_joint_nonstates = anti_join(scribe_joint, list_of_states, by = 'State')

# And now to get our sets for only the 50 states
scribe_joint = scribe_joint[
  !scribe_joint$State %in% scribe_joint_nonstates$State,
  ]
```

### Creating Variable for Total Number of Opioids Prescribed

Since we are working with multiple variables involving opioids, we therefore must avoid the mistake of including an opioid-variable in both the predictor and the outcome variables. Doing so would result in skewness of model accuracy since the predictors and the outcome are directly correlated and would predict an outcome that we already know.

Looking at the `opioid_list` data frame. 
```{r opiod_list}
unique(opioid_list$Generic.Name) # To see which medications are opioids
top_opioids = rbind(
  opioid_list[grep("BUPRENORPHINE", opioid_list$Generic.Name),],
  opioid_list[grep("BUTORPHANOL", opioid_list$Generic.Name),],
  opioid_list[grep("CODEINE", opioid_list$Generic.Name),],
  opioid_list[grep("FENTANYL", opioid_list$Generic.Name),],
  opioid_list[grep("HYDROCODONE", opioid_list$Generic.Name),],
  opioid_list[grep("HYDROMORPHONE", opioid_list$Generic.Name),],
  opioid_list[grep("LEVORPHANOL", opioid_list$Generic.Name),],  
  opioid_list[grep("MEPERIDINE", opioid_list$Generic.Name),],
  opioid_list[grep("METHADONE", opioid_list$Generic.Name),],
  opioid_list[grep("MORPHINE", opioid_list$Generic.Name),],
  opioid_list[grep("NALBUPHINE", opioid_list$Generic.Name),],
  opioid_list[grep("OPIUM", opioid_list$Generic.Name),],
  opioid_list[grep("OXYCODONE", opioid_list$Generic.Name),],
  opioid_list[grep("OXYMORPHONE", opioid_list$Generic.Name),],
  opioid_list[grep("PENTAZOCINE", opioid_list$Generic.Name),],
  opioid_list[grep("TRAMADOL", opioid_list$Generic.Name),])
```

Here we can see the drug class variables that are opioids from the original `prescriber_df`. Next we can use these different drug types to count the total number of prescriptions written by each provider.

```{r summ_var}
working_df = scribe_joint %>%
  select(contains(c("NPI","Gender","State","Credentials","Specialty",
                  "HYDROCODONE","OXYCONTIN",
                  "CODEINE","FENTANYL","PENTAZOCINE",
                  "DIHYDROCODEINE","Opioid.Prescriber",
                  "OPIUM","BUPRENORPHINE","BUTALBIT","BUTORPHANOL",
                  "TRAMADOL","MEPERIDINE","HYDROMORPHONE","METHADONE",
                  "MORPHINE","OXYCODONE","LEVORPHANOL","NALBUPHINE",
                  "TAPENTADOL","OXYMORPHONE","OPIUM")))

# Now we can calculate the sum of opioid prescription written by each provider for the year
working_df$summ_var = rowSums(working_df[ ,c(6:18)])

# adding that column to our original data frame
scribe_joint$summ_var <- working_df$summ_var
```

### Removing Outliers

Looking at the `summ_var` variable that we created, we can see that there is one provider in particular that prescribed a significantly higher amount than any other providers. Let's remove that observation from our data.

```{r rm_outlier}
scribe_joint = subset(scribe_joint, summ_var<4000)
```


### Creating Factor Variables

Thinking back to the overview of the data, 
```{r}
scribe_joint = scribe_joint %>%
  mutate(Gender = as.factor(Gender),
         State  = as.factor(State),
         Credentials = as.factor(Credentials),
         Specialty   = as.factor(Specialty))
```

### Adjustments for Population
```{r}
# Creating a variable for per capita rates
overdose_df$Deaths <- as.numeric(gsub(",","",overdose_df$Deaths))
overdose_df$Population <- as.numeric(
  gsub(",","",overdose_df$Population))
    # Removing commas first to avoid errors in math
overdose_df$deaths_per_capita = (
  overdose_df$Deaths/overdose_df$Population
  )*100000 # per 100 thousand

df = scribe_joint %>%
  group_by(State) %>%
  summarise(state_opioid_volume = sum(summ_var)) %>%
  arrange(State)
overdose_df$state_opioid_volume <- df$state_opioid_volume
    # To give us total opioid prescriptions in each state

# which will allow us to determine the rate-per-thousand people
overdose_df %<>% mutate(opioids_per_cap =
                          (state_opioid_volume/Population)*1000)
```

## Data Visualization

```{r plots}
# Differences when grouped by specialty
ggp1 = ggplot(
  scribe_joint,aes(x = Specialty, y = summ_var, fill = Gender)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Specialty",
       y = "Individual Prescriptions Written",
       fill = "Gender",
       title = "Opioid Prescription Rates by Specialty & Gender") +
  theme(plot.title = element_text(hjust = 0.5, face="bold.italic"), 
        axis.title.x = element_text(size = 12),
        axis.text.x = element_text(size = 7, angle=50,margin = margin(t = 20)),
        axis.title.y = element_text(size = 12),
        legend.title = element_text(face="bold", size = 10))
# Differences when grouped by credentials
ggp2 = ggplot(
  scribe_joint,aes(x = Credentials, y = summ_var, fill = Gender)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Credential",
       y = "Individual Prescriptions Written",
       fill = "Gender",
       title = "Opioid Prescription Rates by Credentials & Gender") +
  theme(plot.title = element_text(hjust = 0.5, face="bold.italic"), 
        axis.title.x = element_text(size = 12),
        axis.text.x = element_text(size = 7),
        axis.title.y = element_text(size = 12),
        legend.title = element_text(face="bold", size = 10))
ggp1
ggp2
```

```{r plots2}
ggp3 = scribe_joint %>%
  group_by(Gender) %>%
  summarise(plyr::count(Specialty))%>%
  ggplot(.,aes(x = x, y = freq, fill = Gender)) +
  geom_bar(stat = "identity", position = "dodge")+
  labs(x = "Specialty",
       y = "Number of Providers",
       fill = "Gender",
       title = "Number of Providers by Specialty & Gender") +
  theme(plot.title = element_text(hjust = 0.5, face="bold.italic"), 
        axis.title.x = element_text(size = 12),
        axis.text.x = element_text(size = 7, angle=50,margin = margin(t = 20)),
        axis.title.y = element_text(size = 12),
        legend.title = element_text(face="bold", size = 10))

ggp4 = scribe_joint %>%
  group_by(Gender) %>%
  summarise(plyr::count(Credentials))%>%
  ggplot(.,aes(x = x, y = freq, fill = Gender)) +
  geom_bar(stat = "identity", position = "dodge")+
  labs(x = "Credential",
       y = "Number of Providers",
       fill = "Gender",
       title = "Number of Providers by Credentials & Gender") +
  theme(plot.title = element_text(hjust = 0.5, face="bold.italic"), 
        axis.title.x = element_text(size = 12),
        axis.text.x = element_text(size = 7),
        axis.title.y = element_text(size = 12),
        legend.title = element_text(face="bold", size = 10))

ggp3
ggp4

```

```{r death_rates}
library(usmap)
library(ggpattern)
statepop <- statepop %>% filter(abbr!='DC')
statepop$per_cap_death <- overdose_df$deaths_per_capita
  
map_of_deaths <- plot_usmap(data = statepop, values = "per_cap_death", color = "black")+
  labs(title = "Deaths From Opioid Overdose in US",
               subtitle = "Population per 100,000")+
  theme(plot.title = element_text(face="bold", size = 14),
        plot.subtitle = element_text(face = "italic"),
        legend.title = element_text(face = "bold", size = 8),
        legend.position = c(0.9, 0.15))+
  scale_fill_continuous(
    low = "white", high = "red", name = "Death Rate (2014)")
map_of_deaths
```

```{r per_cap}
plot_overdose_script = ggplot(
  overdose_df, aes(x=opioids_per_cap, y=deaths_per_capita)) + 
  geom_point(size=4, alpha=.6, color="black")+
  geom_smooth(method = 'lm', se=F, color="red")+
  geom_smooth(method = "loess", se=F, color="blue")+
  xlim(0,102)+
  ylab("Overdose Death Rate (per 100,000)")+
  xlab("Opioid Prescriptions Written (per 1,000 people)")+
  labs(title = 
         "State Overdose Death Rate & Number of Opioid Prescrptiions")
plot_overdose_script
```

# Training & Testing

Now that the data has been cleaned and we have seen it graphically, we can begin to train a model to fit the data. There are several different ways that this data could be modeled, we will be focusing on a couple of models 
### Partioining of Data for Training and Testing
```{r train_test}
set.seed(1234) # Setting seed for reproducibility
train_df = scribe_joint %>%
  sample_frac(0.8) # Partitioning 20% to be used for testing 
test_df  = anti_join(
  scribe_joint, train_df, by = 'NPI')

# Now lets remove the `NPI` variable to make work easier henceforth
train_df = train_df %>% select(-c("NPI"))
test_df = test_df %>% select(-c("NPI"))

```

## *Model 1*: Multivariate Regression

```{r}
# Let's make a copy of the train/test set that we can play with for this model
m1_train <- train_df
m1_test  <- test_df
```

If we want to answer this question, we should remove the variables of the individual opioids and only include the `summ_var` variable we created for total opioids prescribed. Removing the additional variables would prevent double-counting in the data.

```{r}
# Using variable from working_df from before to get list of variables to remove from m1_train
colnames(working_df)
m1_train = m1_train %>%
  select(-c("HYDROCODONE.ACETAMINOPHEN",
            "OXYCONTIN","ACETAMINOPHEN.CODEINE",
            "FENTANYL","Opioid.Prescriber",
            "TRAMADOL.HCL","HYDROMORPHONE.HCL",
            "METHADONE.HCL","MORPHINE.SULFATE",
            "MORPHINE.SULFATE.ER",
            "OXYCODONE.ACETAMINOPHEN",
            "OXYCODONE.HCL"))
```

```{r stepwise_selection}
p_load(tidyverse, caret, leaps)

set.seed(123)
# Forward step wise selection
fwd_step = train(
  summ_var ~.,
  data = m1_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "leapForward",
  tuneGrid = expand.grid(nvmax = 1:15)
)
fwd_step$results
fwd_step$bestTune

# Backward step wise selection
back_step = train(
  summ_var ~.,
  data = m1_train,
  trControl = trainControl(method = "cv", number = 5),
  method = "leapBackward",
  tuneGrid = expand.grid(nvmax = 1:15)
)
back_step$results
back_step$r
```

```{r coef_selection}
fin_fwd_step_list = as.data.frame(coef(fwd_step$finalModel,12))
fin_bck_step_list = as.data.frame(coef(back_step$finalModel,13))
print(fin_bck_step_list)
print(fin_fwd_step_list)
```

Now that we have a list of variables, let's think about which to include in our model. The Orthopedic Surgery  specialty is considerably significant in both models, so let's create a dummy variable indicating if a provider is an orthopedic surgeon. `CARISOPRODOL`, `CYCLOBENZAPRINE.HCL`, `METHOCARBAMOL`, `ONDANSETRON.HCL`, `LYRICA`, `TIZANIDINE.HCL`, `LORAZEPAM`,`PREDNISONE`, and `GABAPENTIN`. These are all the different drugs that appeared in both best-selected models.

`DIVALPROEX.SODIUM.ER` was a negative indicator in both models, so we will also include it in our final model. 

```{r warning=FALSE}
m1_train = m1_train %>%
  mutate(orth_surg_dummy = as.factor(
    if_else(Specialty == "Orthopedic Surgery",1,0))
  )

mod1 = lm(data = m1_train, method = "lm", summ_var~
           orth_surg_dummy + CARISOPRODOL +
           CYCLOBENZAPRINE.HCL+METHOCARBAMOL+
           ONDANSETRON.HCL+LYRICA+TIZANIDINE.HCL+
            LORAZEPAM+PREDNISONE+GABAPENTIN)
summary(mod1)
```

Looking at some measurements of model accuracy

```{r}
m1_train$resid <- resid(mod1)
m1_train$fitted <- fitted(mod1)

sqrt(mean((m1_train$summ_var - m1_train$fitted)^2)) # to calculate MSE
```

Not the best-performing model, but let's see nonetheless how it performs when facing the testing set. Perhaps the insight to learn is that we cannot successfully predict opioid prescription rates from these lists of other prescribed medications

## *Model 2*: Decision Trees  

Are we able to predict the Specialty of a provider based on prescriptions rates and other variables?

```{r}
library(rpart.plot)
m2_train <- train_df %>% na.omit(.)
set.seed(1234)
default_cv = m2_train %>% vfold_cv(v =5)
# Define the decision tree
default_tree = decision_tree(mode ="classification",
                             cost_complexity = tune(),
                             tree_depth = tune()) %>%
  set_engine("rpart")
# Defining recipe
default_recipe = recipe(Gender ~., data = m2_train)
# Defining workflow
default_flow = workflow() %>%
  add_model(default_tree) %>%
  add_recipe(default_recipe)
# Tuning
default_cv_fit = default_flow %>%
  tune_grid(
    default_cv,
    grid = expand_grid(
      cost_complexity = seq(0, 0.15, by = 0.01),
      tree_depth = c(1,2,5,10),
    ),
    metrics = metric_set(accuracy, roc_auc)
  )
# Fitting the best model
best_flow = default_flow %>%
  finalize_workflow(select_best(default_cv_fit, metric = "accuracy")) %>%
  fit(data = m2_train)
# Choosing the best model
best_tree = best_flow %>% extract_fit_parsnip()
# Plotting the tree
best_tree$fit %>% rpart.plot(roundint = FALSE)
```

## *Model 3*: Binscatter Random Forest 
```{r}
m3_train <- train_df
m3_test <- test_df
# Adding indicator vriable for Surgons
m3_train %<>%
  mutate(orth_surg_dummy = as.factor(
    if_else(Specialty == "Orthopedic Surgery",1,0)))
m3_test %<>%
  mutate(orth_surg_dummy = as.factor(
    if_else(Specialty == "Orthopedic Surgery",1,0)))

# Splitting data into bins
m3_train %<>% mutate(new_bin = ntile(summ_var, n=30)) %>%
   mutate(new_bin = as.factor(new_bin))
str(m3_train$new_bin)
m3_train = m3_train %>%
   group_by(new_bin) %>%
   mutate(avg_sum_var = mean(summ_var))
```


The training accuracy for this model is 61.4%.

## *Model 4*: Support Vector Machine (SVM)

For this model, let's see if we can accurately predict the gender of a provider.
```{r}
m4_train <- train_df
m4_test <- test_df

pacman::p_load(e1071)
```

Now to prepare the data for making our predictions of interest
```{r}
# Defining our variables of prediction and outcome
x = m4_train[,-1]
y = m4_train[1]
```

Now running the model (takes a while, be patient)
```{r}
svm_model = svm(Gender ~ .,
                 data = m4_train)
summary(svm_model)

  
preds = predict(svm_model,x)
# Ensure caret package is loaded for line below
confusionMatrix(preds, y$Gender)
```

```{r}
summary(svm_model)
```


# Testing
## *Model 1*: Multivariate Regression
```{r}
m1_test = m1_test %>%
  mutate(orth_surg_dummy = as.factor(
    if_else(Specialty == "Orthopedic Surgery",1,0))
  )
# Using the variables selected with stepwise algorithm
mod1t = lm(data = m1_test, summ_var~
           orth_surg_dummy + CARISOPRODOL +
           CYCLOBENZAPRINE.HCL+METHOCARBAMOL+
           ONDANSETRON.HCL+LYRICA+TIZANIDINE.HCL+
            LORAZEPAM+PREDNISONE+GABAPENTIN)
summary(mod1t)

m1_test$resid <- resid(mod1t)
m1_test$fitted <- fitted(mod1t)
sqrt(mean((m1_test$summ_var - m1_test$fitted)^2)) # to calculate MSE
```

Looking at the testing results, while the model may not be very accurate, it is at least consistently inaccurate when looking at the data. Looking at the plot below, we can see a wide range of values of predicted versus actual, but the linear line of approximation 

```{r pred_vs_actual}
plot(x=m1_test$fitted,
     m1_test$summ_var,
     xlab = "Predicted Values",
     ylab = "Observed Values",
     main = "Linear Model")
abline(a = 0,                                        
       b = 1,
       col = "red",
       lwd = 2)
```

## *Model 2*: Decision Tree

```{r}
m2_test <- test_df %>% na.omit(.)
set.seed(1234)
default_cv = m2_test %>% vfold_cv(v =5)
default_recipe = recipe(Gender ~., data = m2_test)
# Defining workflow
default_flow = workflow() %>%
  add_model(default_tree) %>%
  add_recipe(default_recipe)
# Tuning
default_cv_fit = default_flow %>%
  tune_grid(
    default_cv,
    grid = expand_grid(
      cost_complexity = seq(0, 0.15, by = 0.01),
      tree_depth = c(1,2,5,10),
    ),
    metrics = metric_set(accuracy, roc_auc)
  )
# Fitting the best model
best_flow = default_flow %>%
  finalize_workflow(select_best(default_cv_fit, metric = "accuracy")) %>%
  fit(data = m2_test)
# Choosing the best model
best_tree = best_flow %>% extract_fit_parsnip()
# Plotting the tree
best_tree$fit %>% rpart.plot(roundint = FALSE)
```

## *Model 3*: Random Forest

```{r}
m3_test %<>% mutate(new_bin = ntile(summ_var, n=30)) %>%
   mutate(new_bin = as.factor(new_bin)) %>%
   group_by(new_bin) %>%
   mutate(avg_sum_var = mean(summ_var))

rf_mod = randomForest::randomForest(formula = new_bin ~.,
                                    data = m3_train,
                                    ntree = 50)
m3_test$pred_rf = predict(rf_mod, newdata = m3_test, type = "response")

# to obtain the number of times the predicted values were the actual values
length(which(m3_test$pred_rf==m3_test$new_bin)) -> accurate #1758
# divided by total observations to obtain accuracy
accurate / (nrow(m3_test))
```

## *Model 4*: Support Vector Machine

Using the same SVM approach as with the testing set.
```{r svm_testing}
xt = m4_test[,-1]
yt = m4_test[1]

svm_model_test = svm(Gender ~ .,
                 data = m4_test)
summary(svm_model_test)

  
predst = predict(svm_model_test,xt)
# Ensure caret package is loaded for line below
confusionMatrix(predst, yt$Gender)
```

Again, the results are comparable. 

# Conclusions

The linear model predicting the `summ_var` variable did a decent job at fitting the general trend in the data, but had a Mean Squared Error (MSE) which might suggest some concern. But considering the range of the values was around 3000, that is a pretty small margin in the context. It was also reassuring to see that the same general trend existed in both the training and the testing sets 

```{r}
plot(x=m1_test$fitted,
     m1_test$summ_var,
     xlab = "Predicted Values",
     ylab = "Observed Values",
     main = "Linear Model")
abline(a = 0,                                        
       b = 1,
       col = "red",
       lwd = 2)
```

We can go one step further and create a binned scatter plot of the actual versus predicted values and see that the linear fit becomes stronger
```{r}
p1= ggplot(m1_test, aes(x=fitted,y=summ_var)) +
  geom_point(size=2, alpha = 0.1, color="red") +
  stat_summary_bin(fun.y='mean', bins=75, 
                   color='blue', size=1.5, geom='point')+
   geom_smooth(method = "lm", color="blue",se=F)+
   theme(axis.title.x = element_blank(),
         axis.title.y = element_blank())

p2 = ggplot(m1_test, aes(x=fitted,y=summ_var)) +
  geom_point(size=2, alpha = 0.1, color="red") +
  stat_summary_bin(fun.y='mean', bins=15, 
                   color='blue', size=1.5, geom='point')+
   geom_smooth(method = "lm", color="blue",se=F)+
   theme(axis.title.x = element_blank(),
         axis.title.y = element_blank())

ggpubr::ggarrange(p1,p2)
```

Especially if we change the number of bins used

```{r}
p3= ggplot(m1_test, aes(x=fitted,y=summ_var)) +
  geom_point(size=2, alpha = 0.1, color="red") +
  stat_summary_bin(fun.y='mean', bins=75, 
                   color='blue', size=1.5, geom='point')+
   geom_smooth(method = "lm", color="blue",se=F)+
   theme(axis.title.x = element_blank(),
         axis.title.y = element_blank())+
      xlim(0,2000)+ylim(0,2000)

p4= ggplot(m1_test, aes(x=fitted,y=summ_var)) +
  geom_point(size=2, alpha = 0.1, color="red") +
  stat_summary_bin(fun.y='mean', bins=15, 
                   color='blue', size=1.5, geom='point')+
   geom_smooth(method = "lm", color="blue",se=F)+
   theme(axis.title.x = element_blank(),
         axis.title.y = element_blank())+
   xlim(0,2000)+ylim(0,2000)

ggpubr::ggarrange(p3,p4)

```


The Random forest model we used has an accuracy of 61.4%. Compared to the The SVM model had an accuracy of 77.37% (and with a 95% confidence interval of (0.7579, 0.7889)).The decision tree model had an accuracy of 73.94% as calculated from the confusion matrix below:

```{r}
m2_test$tree_mod = predict(best_tree, new_data =  m2_test)
temp_df = m2_test %>% rename(tree_mod = 257) %>%
   mutate(tree_pred = if_else(tree_mod =="M", "MALE","FEMALE" ))
table(Actual = temp_df$Gender,
      Predicted = temp_df$tree_pred)
(522+1595)/(522+1595+500+246)
```

Accuracy was not a metric that was worth out consideration when predicting continuous variables. Due to the natural smoothing that result from modeling, it is not very likely that the model will get many (if any) predictions that correct down to the integer.
 
There are also other measurements of accuracy in the models above that we care about and are important to consider when building or assessing a model. Looking at a plot of the residuals for the regression model, we can see a strong clustering around 0, but there is still noise uncaptured by the model.

```{r}
plot(m1_test$resid)
```

If we look at the listed variables of importance for the decision tree model, we which different variables were valued more in each model. Knowledge of the construction of these models is important when attempting to use to predict new data where changes in policy or other changes have altered trends in healthcare and opioid prescriptions (like prescription druge monitoring programs).

```{r}
best_tree$fit$variable.importance
```

When it came to the classification prediction of the `gender` variable, the SVM outperformed the decision tree. It would be interested to see how these models would perform when facing more-recent data since the methods that we used to contrast the model should adjust for potential differences in volume by using proportions as a whole
